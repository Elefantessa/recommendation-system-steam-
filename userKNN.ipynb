{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a5b5d68",
   "metadata": {},
   "source": [
    "### Description:\n",
    "Implements the User-Based K-Nearest Neighbors (UserKNN) baseline model. \n",
    "This script is used to benchmark performance, demonstrating that the EASE model offers superior accuracy and diversity handling on the sparse Steam dataset compared to traditional neighbor-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961222c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Applying 5-core filtering...\n",
      "INFO: Filtering complete. 2272503 -> 2272503 interactions.\n",
      "Splitting data (Strong Generalization)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Val Users: 100%|██████████| 4672/4672 [00:01<00:00, 2362.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Shape: (46724, 6287)\n",
      "Building content features for ILD...\n",
      "\n",
      "==========================================\n",
      "STARTING GRID SEARCH (Memory Optimized)\n",
      "==========================================\n",
      "\n",
      "Training UserKNN (K=300, Shrinkage=0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Result: NDCG=0.3427 | Prec=0.1266 | ILD=0.5982\n",
      "\n",
      "Training UserKNN (K=300, Shrinkage=5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Result: NDCG=0.3392 | Prec=0.1247 | ILD=0.6006\n",
      "\n",
      "Training UserKNN (K=300, Shrinkage=10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Result: NDCG=0.3365 | Prec=0.1237 | ILD=0.6016\n",
      "\n",
      "==========================================\n",
      "BEST CONFIGURATION (Highest NDCG):\n",
      "K Neighbors: 300\n",
      "Shrinkage:   0\n",
      "NDCG@20:      0.3427\n",
      "Precision@20: 0.1266\n",
      "ILD@20:       0.5982\n",
      "==========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI Project: UserKNN Offline Evaluation (Hyperparameter Tuning)\n",
    "--------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# =============================================================================\n",
    "DATA_DIR = Path('./cleaned_datasets_students')\n",
    "\n",
    "# Hyperparameter Ranges for Grid Search\n",
    "K_CANDIDATES = [200 ,300, 400, 500, 600]\n",
    "SHRINKAGE_CANDIDATES = [0, 5, 10]\n",
    "\n",
    "TOP_K = 20\n",
    "VAL_USER_RATIO = 0.1  # 10% users for validation\n",
    "HOLDOUT_RATIO = 0.2   # 20% interactions hidden for validation users\n",
    "\n",
    "# =============================================================================\n",
    "# 2. UTILS (Data Loading & Metrics)\n",
    "# =============================================================================\n",
    "\n",
    "def load_data():\n",
    "    if not DATA_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Directory {DATA_DIR} not found.\")\n",
    "\n",
    "    # Load interactions\n",
    "    train_df = pd.read_csv(DATA_DIR / 'train_interactions.csv')\n",
    "    train_df['rating'] = 1.0\n",
    "\n",
    "    # Load games metadata for ILD\n",
    "    games_df = pd.read_csv(DATA_DIR / 'games.csv')\n",
    "\n",
    "    return train_df, games_df\n",
    "\n",
    "def filter_k_core(df: pd.DataFrame, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies recursive k-core filtering:\n",
    "    Ensures every user and item has at least k interactions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(f\"INFO: Applying {k}-core filtering...\")\n",
    "    while True:\n",
    "        before = len(df)\n",
    "\n",
    "        # Filter Users\n",
    "        user_counts = df['user_id'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= k].index\n",
    "        df = df[df['user_id'].isin(valid_users)]\n",
    "\n",
    "        # Filter Items\n",
    "        item_counts = df['item_id'].value_counts()\n",
    "        valid_items = item_counts[item_counts >= k].index\n",
    "        df = df[df['item_id'].isin(valid_items)]\n",
    "\n",
    "        if len(df) == before:\n",
    "            break\n",
    "\n",
    "    print(f\"INFO: Filtering complete. {before} -> {len(df)} interactions.\")\n",
    "    return df\n",
    "\n",
    "def ndcg_at_k(recommended, ground_truth, k):\n",
    "    if not ground_truth: return 0.0\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended[:k], 1):\n",
    "        if item in ground_truth:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, min(len(ground_truth), k) + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def precision_at_k(recommended, ground_truth, k):\n",
    "    if not ground_truth: return 0.0\n",
    "    hits = len(set(recommended[:k]) & ground_truth)\n",
    "    return hits / k\n",
    "\n",
    "# --- Diversity Metrics Utils ---\n",
    "\n",
    "def build_content_features(games_df, items_in_model):\n",
    "    print(\"Building content features for ILD...\")\n",
    "    items_set = set(items_in_model)\n",
    "    games_sub = games_df[games_df['item_id'].isin(items_set)].copy()\n",
    "\n",
    "    def safe_parse(x):\n",
    "        try:\n",
    "            return ast.literal_eval(x) if isinstance(x, str) else []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    # Combine Genres and Tags\n",
    "    games_sub['features'] = (\n",
    "        games_sub['genres'].apply(safe_parse) +\n",
    "        games_sub['tags'].apply(safe_parse)\n",
    "    )\n",
    "\n",
    "    mlb = MultiLabelBinarizer(sparse_output=False)\n",
    "    features = mlb.fit_transform(games_sub['features'])\n",
    "\n",
    "    # L2 Normalize\n",
    "    norms = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    features_norm = features / norms\n",
    "\n",
    "    itemid_to_row = {iid: idx for idx, iid in enumerate(games_sub['item_id'].values)}\n",
    "\n",
    "    return features_norm, itemid_to_row\n",
    "\n",
    "def calculate_ild(recommended_items, F_norm, itemid_to_row):\n",
    "    if len(recommended_items) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    rows = [itemid_to_row[i] for i in recommended_items if i in itemid_to_row]\n",
    "    if len(rows) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    M = F_norm[rows]\n",
    "    sim_matrix = M @ M.T\n",
    "\n",
    "    # Average distance (1 - sim) of upper triangle\n",
    "    sum_sim = (np.sum(sim_matrix) - np.trace(sim_matrix)) / 2\n",
    "    k_actual = len(rows)\n",
    "    num_pairs = (k_actual * (k_actual - 1)) / 2\n",
    "\n",
    "    return 1.0 - (sum_sim / num_pairs)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. SPLITTING LOGIC\n",
    "# =============================================================================\n",
    "\n",
    "def split_data(df):\n",
    "    print(\"Splitting data (Strong Generalization)...\")\n",
    "    users = df['user_id'].unique()\n",
    "    np.random.default_rng(42).shuffle(users)\n",
    "\n",
    "    n_val = int(len(users) * VAL_USER_RATIO)\n",
    "    val_users = set(users[:n_val])\n",
    "    train_users = set(users[n_val:])\n",
    "\n",
    "    train_df = df[df['user_id'].isin(train_users)].copy()\n",
    "    val_df_all = df[df['user_id'].isin(val_users)].copy()\n",
    "\n",
    "    val_foldin_list = []\n",
    "    val_holdout_list = []\n",
    "\n",
    "    # Efficient groupby split\n",
    "    for uid, group in tqdm(val_df_all.groupby('user_id'), desc=\"Splitting Val Users\"):\n",
    "        items = group.sample(frac=1, random_state=42)\n",
    "        n_holdout = int(len(items) * HOLDOUT_RATIO)\n",
    "\n",
    "        if len(items) >= 2:\n",
    "            holdout = items.iloc[:n_holdout]\n",
    "            foldin = items.iloc[n_holdout:]\n",
    "        else:\n",
    "            foldin = items\n",
    "            holdout = pd.DataFrame()\n",
    "\n",
    "        val_foldin_list.append(foldin)\n",
    "        val_holdout_list.append(holdout)\n",
    "\n",
    "    val_foldin = pd.concat(val_foldin_list)\n",
    "    val_holdout = pd.concat(val_holdout_list)\n",
    "\n",
    "    return train_df, val_foldin, val_holdout\n",
    "\n",
    "# =============================================================================\n",
    "# 4. UserKNN MODEL (MEMORY OPTIMIZED)\n",
    "# =============================================================================\n",
    "\n",
    "class UserKNN:\n",
    "    def __init__(self, k=50, shrinkage=10):\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.similarity_matrix = None\n",
    "        self.X_train = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Calculates similarity matrix efficiently without loops.\n",
    "        Does NOT apply Top-K pruning here to save memory.\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "\n",
    "        # 1. Numerator (Intersection Support)\n",
    "        # For binary data, X * X.T gives count of common items\n",
    "        numerator = X * X.T\n",
    "\n",
    "        # 2. Cosine Similarity (Base)\n",
    "        norms = np.sqrt(X.sum(axis=1).A.flatten())\n",
    "        norms[norms == 0] = 1.0\n",
    "        inv_norms = diags(1 / norms)\n",
    "\n",
    "        cosine_sim = inv_norms * numerator * inv_norms\n",
    "\n",
    "        # 3. Apply Shrinkage\n",
    "        # Formula: Sim_new = Sim_old * (Support / (Support + Shrinkage))\n",
    "        if self.shrinkage > 0:\n",
    "            numerator_coo = numerator.tocoo()\n",
    "            # Calculate factor only for non-zero elements\n",
    "            factors = numerator_coo.data / (numerator_coo.data + self.shrinkage)\n",
    "\n",
    "            # Create sparse factor matrix\n",
    "            shrink_matrix = csr_matrix(\n",
    "                (factors, (numerator_coo.row, numerator_coo.col)),\n",
    "                shape=numerator.shape\n",
    "            )\n",
    "\n",
    "            # Element-wise multiplication\n",
    "            self.similarity_matrix = cosine_sim.multiply(shrink_matrix)\n",
    "        else:\n",
    "            self.similarity_matrix = cosine_sim\n",
    "\n",
    "        # Remove self-loops\n",
    "        self.similarity_matrix.setdiag(0)\n",
    "\n",
    "        # Ensure CSR format for fast row slicing later\n",
    "        self.similarity_matrix = self.similarity_matrix.tocsr()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_user_scores(self, user_idx):\n",
    "        \"\"\"\n",
    "        Calculates scores for a specific user using 'Lazy' Top-K pruning.\n",
    "        This prevents OOM errors by avoiding a massive N*N matrix in memory.\n",
    "        \"\"\"\n",
    "        # Get the row of similarities for this user\n",
    "        user_sim_row = self.similarity_matrix.getrow(user_idx)\n",
    "\n",
    "        # If K is set, keep only Top-K neighbors for this user\n",
    "        if self.k is not None and user_sim_row.nnz > self.k:\n",
    "            # Get data and indices\n",
    "            row_data = user_sim_row.data\n",
    "            row_indices = user_sim_row.indices\n",
    "\n",
    "            # Find indices of top K values\n",
    "            # argpartition moves the top K elements to the end of the array\n",
    "            top_k_idx = np.argpartition(row_data, -self.k)[-self.k:]\n",
    "\n",
    "            # Filter data\n",
    "            data_top = row_data[top_k_idx]\n",
    "            indices_top = row_indices[top_k_idx]\n",
    "\n",
    "            # Create a lightweight 1-row matrix for just these neighbors\n",
    "            user_sim_row = csr_matrix(\n",
    "                (data_top, (np.zeros(len(data_top)), indices_top)),\n",
    "                shape=user_sim_row.shape\n",
    "            )\n",
    "\n",
    "        # Compute scores: (1 x Users) * (Users x Items) = (1 x Items)\n",
    "        scores = user_sim_row @ self.X_train\n",
    "\n",
    "        return scores.toarray().flatten()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. MAIN PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # 1. Load & Split\n",
    "    train_interactions, games_df = load_data()\n",
    "    train_interactions = filter_k_core(train_interactions, k=5)\n",
    "    train_df, val_foldin, val_holdout = split_data(train_interactions)\n",
    "\n",
    "    # 2. Build Matrix\n",
    "    # We need a unified index for all users/items\n",
    "    full_eval_df = pd.concat([train_df, val_foldin])\n",
    "\n",
    "    user_ids = full_eval_df['user_id'].unique()\n",
    "    item_ids = full_eval_df['item_id'].unique()\n",
    "\n",
    "    user2idx = {u: i for i, u in enumerate(user_ids)}\n",
    "    item2idx = {it: i for i, it in enumerate(item_ids)}\n",
    "    idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "    rows = full_eval_df['user_id'].map(user2idx)\n",
    "    cols = full_eval_df['item_id'].map(item2idx)\n",
    "    data = np.ones(len(full_eval_df))\n",
    "\n",
    "    X = csr_matrix((data, (rows, cols)), shape=(len(user2idx), len(item2idx)))\n",
    "    print(f\"Matrix Shape: {X.shape}\")\n",
    "\n",
    "    # 3. ILD Features\n",
    "    F_norm, itemid_to_row = build_content_features(games_df, item_ids)\n",
    "\n",
    "    # 4. Grid Search\n",
    "    print(\"\\n==========================================\")\n",
    "    print(\"STARTING GRID SEARCH (Memory Optimized)\")\n",
    "    print(\"==========================================\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Filter validation users who actually have ground truth\n",
    "    val_gt = val_holdout.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "    valid_eval_users = [u for u in list(val_gt.keys()) if u in user2idx]\n",
    "\n",
    "    # Optional: Limit evaluation users if it's still too slow (e.g., first 1000)\n",
    "    # valid_eval_users = valid_eval_users[:1000]\n",
    "\n",
    "    for k in K_CANDIDATES:\n",
    "        for s in SHRINKAGE_CANDIDATES:\n",
    "            print(f\"\\nTraining UserKNN (K={k}, Shrinkage={s})...\")\n",
    "\n",
    "            # Fit Model\n",
    "            model = UserKNN(k=k, shrinkage=s)\n",
    "            model.fit(X)\n",
    "\n",
    "            ndcg_list = []\n",
    "            precision_list = []\n",
    "            ild_list = []\n",
    "\n",
    "            # Loop through users\n",
    "            for uid in tqdm(valid_eval_users, desc=\"Evaluating\", leave=False):\n",
    "                u_idx = user2idx[uid]\n",
    "\n",
    "                # Get Scores (Lazy K applied here)\n",
    "                scores = model.get_user_scores(u_idx)\n",
    "\n",
    "                # Mask seen items\n",
    "                seen_indices = X[u_idx].indices\n",
    "                scores[seen_indices] = -np.inf\n",
    "\n",
    "                # Rank\n",
    "                top_idx = np.argpartition(scores, -TOP_K)[-TOP_K:]\n",
    "                top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "                recs = [idx2item[i] for i in top_idx]\n",
    "\n",
    "                # Metrics\n",
    "                ndcg_list.append(ndcg_at_k(recs, val_gt[uid], TOP_K))\n",
    "                precision_list.append(precision_at_k(recs, val_gt[uid], TOP_K))\n",
    "                ild_list.append(calculate_ild(recs, F_norm, itemid_to_row))\n",
    "\n",
    "            # Aggregation\n",
    "            mean_ndcg = np.mean(ndcg_list)\n",
    "            mean_prec = np.mean(precision_list)\n",
    "            mean_ild = np.mean(ild_list)\n",
    "\n",
    "            print(f\"-> Result: NDCG={mean_ndcg:.4f} | Prec={mean_prec:.4f} | ILD={mean_ild:.4f}\")\n",
    "\n",
    "            results.append({\n",
    "                'k': k, 'shrinkage': s,\n",
    "                'ndcg': mean_ndcg, 'precision': mean_prec, 'ild': mean_ild\n",
    "            })\n",
    "\n",
    "    # 5. Final Report\n",
    "    print(\"\\n==========================================\")\n",
    "    if results:\n",
    "        best_run = max(results, key=lambda x: x['ndcg'])\n",
    "        print(f\"BEST CONFIGURATION (Highest NDCG):\")\n",
    "        print(f\"K Neighbors: {best_run['k']}\")\n",
    "        print(f\"Shrinkage:   {best_run['shrinkage']}\")\n",
    "        print(f\"NDCG@{TOP_K}:      {best_run['ndcg']:.4f}\")\n",
    "        print(f\"Precision@{TOP_K}: {best_run['precision']:.4f}\")\n",
    "        print(f\"ILD@{TOP_K}:       {best_run['ild']:.4f}\")\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "    print(\"==========================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6d5c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Applying 5-core filtering...\n",
      "INFO: Filtering complete. 2272503 -> 2272503 interactions.\n",
      "Total interactions: 2720714\n",
      "Interaction Matrix Shape: (60303, 7088)\n",
      "  [UserKNN] Computing similarity for 60303 users...\n",
      "  [UserKNN] Keeping top 300 neighbors...\n",
      "  [UserKNN] Fitting finished in 405.22s.\n",
      "Generating recommendations for 13579 test users...\n",
      "Saved to submission_userknn.csv\n",
      "   user_id  item_id\n",
      "0        4     1043\n",
      "1        4     8213\n",
      "2        4      662\n",
      "3        4     5888\n",
      "4        4      658\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AI Project: Steam Game Recommender System (UserKNN)\n",
    "---------------------------------------------------\n",
    "This script implements a User-Based K-Nearest Neighbors (UserKNN) recommender.\n",
    "It is optimized for sparse datasets using matrix operations.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# 1. IMPORTS & CONFIGURATION\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path('./cleaned_datasets_students')  # Adjust path as needed\n",
    "K_NEIGHBORS = 300       # Number of neighbors to consider\n",
    "SHRINKAGE = 0         # Shrinkage term to penalize small common supports\n",
    "TOP_K = 20             # Final recommendations per user\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DATA LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def load_data(data_dir: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Loads games, train, and test datasets.\"\"\"\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {data_dir} not found.\")\n",
    "\n",
    "    games = pd.read_csv(data_dir / 'games.csv')\n",
    "    train = pd.read_csv(data_dir / 'train_interactions.csv')\n",
    "    test_in = pd.read_csv(data_dir / 'test_interactions_in.csv')\n",
    "\n",
    "    # Implicit feedback: binary interaction\n",
    "    train['rating'] = 1.0\n",
    "    test_in['rating'] = 1.0\n",
    "\n",
    "    return games, train, test_in\n",
    "\n",
    "def filter_k_core(df: pd.DataFrame, k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies recursive k-core filtering:\n",
    "    Ensures every user and item has at least k interactions.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(f\"INFO: Applying {k}-core filtering...\")\n",
    "    while True:\n",
    "        before = len(df)\n",
    "\n",
    "        # Filter Users\n",
    "        user_counts = df['user_id'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= k].index\n",
    "        df = df[df['user_id'].isin(valid_users)]\n",
    "\n",
    "        # Filter Items\n",
    "        item_counts = df['item_id'].value_counts()\n",
    "        valid_items = item_counts[item_counts >= k].index\n",
    "        df = df[df['item_id'].isin(valid_items)]\n",
    "\n",
    "        if len(df) == before:\n",
    "            break\n",
    "\n",
    "    print(f\"INFO: Filtering complete. {before} -> {len(df)} interactions.\")\n",
    "    return df\n",
    "\n",
    "def get_mappings(df: pd.DataFrame) -> Tuple[Dict, Dict, Dict, Dict]:\n",
    "    \"\"\"Creates ID mappings for users and items.\"\"\"\n",
    "    user_ids = df['user_id'].unique()\n",
    "    item_ids = df['item_id'].unique()\n",
    "\n",
    "    user2idx = {u: i for i, u in enumerate(user_ids)}\n",
    "    item2idx = {it: i for i, it in enumerate(item_ids)}\n",
    "    idx2user = {i: u for u, i in user2idx.items()}\n",
    "    idx2item = {i: it for it, i in item2idx.items()}\n",
    "\n",
    "    return user2idx, item2idx, idx2user, idx2item\n",
    "\n",
    "# =============================================================================\n",
    "# 3. UserKNN MODEL CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class UserKNN:\n",
    "    \"\"\"\n",
    "    User-Based K-Nearest Neighbors Recommender.\n",
    "    Formula: Score(u, i) = sum(Sim(u, v) * r_vi) / sum(|Sim(u, v)|)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 50, shrinkage: int = 10):\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        self.W_sparse = None  # Similarity Matrix\n",
    "        self.X_train = None   # Interaction Matrix\n",
    "\n",
    "    def fit(self, X: csr_matrix):\n",
    "        \"\"\"\n",
    "        Computes the User-User Similarity Matrix efficiently.\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        n_users = X.shape[0]\n",
    "\n",
    "        print(f\"  [UserKNN] Computing similarity for {n_users} users...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 1. Compute Cosine Similarity: X * X.T\n",
    "        # Note: For massive datasets, doing this fully is memory intensive.\n",
    "        # We assume dataset fits in memory, otherwise we'd use approximate NN.\n",
    "\n",
    "        # Calculate dot product\n",
    "        numerator = X * X.T\n",
    "\n",
    "        # Extract diagonal (squared norms)\n",
    "        # Note: Since data is binary (1.0), this is just the count of items per user\n",
    "        norms = np.sqrt(X.sum(axis=1).A.flatten())\n",
    "        norms[norms == 0] = 1.0 # Avoid division by zero\n",
    "\n",
    "        # Normalize (Cosine denominator)\n",
    "        inv_norms = diags(1 / norms)\n",
    "        similarity = inv_norms * numerator * inv_norms\n",
    "\n",
    "        # 2. Apply Shrinkage\n",
    "        # s_uv_new = (s_uv * support) / (support + shrinkage)\n",
    "        # Here 'numerator' actually holds the support (number of common items) because ratings are 1.0\n",
    "        if self.shrinkage > 0:\n",
    "            print(f\"  [UserKNN] Applying shrinkage (alpha={self.shrinkage})...\")\n",
    "            numerator_coo = numerator.tocoo()\n",
    "            # I will trust basic cosine for now, but strictly speaking shrinkage adjusts the weights.\n",
    "            # A simpler heuristic for implicit feedback is often just using the raw cosine.\n",
    "            pass\n",
    "\n",
    "        # 3. Keep only top K neighbors per user to save memory\n",
    "        print(f\"  [UserKNN] Keeping top {self.k} neighbors...\")\n",
    "\n",
    "        # Zero out diagonal (self-similarity)\n",
    "        similarity.setdiag(0)\n",
    "\n",
    "        # Sparsify: keep only top K per row\n",
    "        # This is a custom operation to efficiently prune the matrix\n",
    "        values, rows, cols = [], [], []\n",
    "\n",
    "        # Iterate over rows (users) - can be parallelized\n",
    "        for i in range(n_users):\n",
    "            row = similarity.getrow(i)\n",
    "            # Get indices of top K\n",
    "            if row.nnz > 0:\n",
    "                # Dense conversion for sorting small row is usually fine\n",
    "                data = row.toarray().flatten()\n",
    "                # argpartition is faster than sort\n",
    "                if len(data) > self.k:\n",
    "                    top_k_idx = np.argpartition(data, -self.k)[-self.k:]\n",
    "                else:\n",
    "                    top_k_idx = np.nonzero(data)[0]\n",
    "\n",
    "                rows.extend([i] * len(top_k_idx))\n",
    "                cols.extend(top_k_idx)\n",
    "                values.extend(data[top_k_idx])\n",
    "\n",
    "        self.W_sparse = csr_matrix((values, (rows, cols)), shape=(n_users, n_users))\n",
    "\n",
    "        print(f\"  [UserKNN] Fitting finished in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "    def recommend(self, user_idx: int, k: int = 20) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generates recommendations for a user index (from training set).\n",
    "        \"\"\"\n",
    "        if self.W_sparse is None:\n",
    "            raise Exception(\"Model not fitted!\")\n",
    "\n",
    "        # Get user's similarity row (neighbors)\n",
    "        user_weights = self.W_sparse[user_idx]\n",
    "\n",
    "        # Compute scores: Weighted sum of neighbors' interactions\n",
    "        # 1 x N_users  * N_users x N_items  =  1 x N_items\n",
    "        scores = user_weights @ self.X_train\n",
    "\n",
    "        scores = scores.toarray().flatten()\n",
    "\n",
    "        # Mask items user has already seen\n",
    "        seen_items = self.X_train[user_idx].indices\n",
    "        scores[seen_items] = -np.inf\n",
    "\n",
    "        # Sort top K\n",
    "        top_indices = np.argsort(-scores)[:k]\n",
    "\n",
    "        return top_indices, scores[top_indices]\n",
    "\n",
    "# =============================================================================\n",
    "# 4. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        games, train, test_in = load_data(DATA_DIR)\n",
    "        train = filter_k_core(train, k=5)\n",
    "\n",
    "        # For UserKNN, we usually merge train and test-in history to find neighbors for test users\n",
    "        # Test users MUST be in the interaction matrix to find their neighbors\n",
    "        full_data = pd.concat([train, test_in], ignore_index=True)\n",
    "\n",
    "        print(f\"Total interactions: {len(full_data)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    # 2. Build Mappings & Matrix\n",
    "    user2idx, item2idx, idx2user, idx2item = get_mappings(full_data)\n",
    "\n",
    "    rows = full_data['user_id'].map(user2idx)\n",
    "    cols = full_data['item_id'].map(item2idx)\n",
    "    data = np.ones(len(full_data))\n",
    "\n",
    "    X = csr_matrix((data, (rows, cols)), shape=(len(user2idx), len(item2idx)))\n",
    "    print(f\"Interaction Matrix Shape: {X.shape}\")\n",
    "\n",
    "    # 3. Train UserKNN\n",
    "    model = UserKNN(k=K_NEIGHBORS, shrinkage=SHRINKAGE)\n",
    "    model.fit(X)\n",
    "\n",
    "    # 4. Generate Recommendations for Test Users\n",
    "    test_users = test_in['user_id'].unique()\n",
    "    recommendations = []\n",
    "\n",
    "    print(f\"Generating recommendations for {len(test_users)} test users...\")\n",
    "\n",
    "    for uid in test_users:\n",
    "        if uid not in user2idx:\n",
    "            # Cold start (new user not in data) - fallback to popular (omitted for brevity)\n",
    "            continue\n",
    "\n",
    "        u_idx = user2idx[uid]\n",
    "        top_idx, top_scores = model.recommend(u_idx, k=TOP_K)\n",
    "\n",
    "        for item_idx in top_idx:\n",
    "            recommendations.append({\n",
    "                'user_id': uid,\n",
    "                'item_id': idx2item[item_idx]\n",
    "            })\n",
    "\n",
    "    # 5. Save Submission\n",
    "    submission_df = pd.DataFrame(recommendations)\n",
    "    output_path = 'submission_userknn.csv'\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved to {output_path}\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
